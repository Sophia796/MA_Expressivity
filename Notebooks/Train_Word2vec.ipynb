{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba672df",
   "metadata": {},
   "source": [
    "# Trainingspipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e5260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "import spacy\n",
    "\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from joblib import Parallel, delayed  \n",
    "from nltk.corpus import stopwords\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09922b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('italian')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/italian.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f3e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Korpus/Korpus/corpus_final.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312fc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'lemmatized text': 'lemmatized_text', 'cleaned tokenized text': 'cleaned_tokenized_text'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8fa580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>text type</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Espositivi.IV.4.Testo.txt</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>Ludovico Antonio Muratori</td>\n",
       "      <td>Antichità italiane</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700-1750</td>\n",
       "      <td>espositivo</td>\n",
       "      <td>﻿IV. 4. Ludovico Antonio Muratori, Antichità i...</td>\n",
       "      <td>8990.0</td>\n",
       "      <td>﻿iv . 4 . Ludovico Antonio muratori , antichit...</td>\n",
       "      <td>[['iv'], [], ['ludovico', 'antonio', 'muratori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Poesia.IV.1.Testo.txt</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>Giuseppe Paolucci (Alessi Cillenio)</td>\n",
       "      <td>Poesie</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700-1750</td>\n",
       "      <td>poesia</td>\n",
       "      <td>IV. 1. Rime degli Arcadi: Alessi Cillenio (Giu...</td>\n",
       "      <td>10862.0</td>\n",
       "      <td>iv . 1 . rima del arcadi : alessi cillenio ( G...</td>\n",
       "      <td>[['iv'], [], ['rima', 'arcadi', 'alessi', 'cil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Personali.IV.4.Testo.txt</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>Vincenzo da Filicaia</td>\n",
       "      <td>Lettere inedite a Lorenzo Magalotti</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700-1750</td>\n",
       "      <td>personale</td>\n",
       "      <td>IV. 4. Vincenzo da Filicaia, Lettere inedite a...</td>\n",
       "      <td>10073.0</td>\n",
       "      <td>iv . 4 . Vincenzo da filicaia , lettere inedit...</td>\n",
       "      <td>[['iv'], [], ['vincenzo', 'filicaia', 'lettere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Personali.IV.5.Testo.txt</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>Lorenzo Magalotti</td>\n",
       "      <td>Lettere odorose (1693-1705)</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700-1750</td>\n",
       "      <td>personale</td>\n",
       "      <td>IV. 5. Lorenzo Magalotti, Lettere odorose (169...</td>\n",
       "      <td>8374.0</td>\n",
       "      <td>iv . 5 . Lorenzo magalotti , lettere odoroso (...</td>\n",
       "      <td>[['iv'], [], ['lorenzo', 'magalotti', 'lettere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Poesia.IV.4.Testo.txt</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>Faustina Maratti Zappi</td>\n",
       "      <td>Poesie</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700-1750</td>\n",
       "      <td>poesia</td>\n",
       "      <td>IV. 4. Rime degli Arcadi: Aglauro Cidonia (Fau...</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>iv . 4 . rima del arcadi : aglauro cidonia ( f...</td>\n",
       "      <td>[['iv'], [], ['rima', 'arcadi', 'aglauro', 'ci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         doc source                               author  \\\n",
       "0  Espositivi.IV.4.Testo.txt  MIDIA           Ludovico Antonio Muratori    \n",
       "1      Poesia.IV.1.Testo.txt  MIDIA  Giuseppe Paolucci (Alessi Cillenio)   \n",
       "2   Personali.IV.4.Testo.txt  MIDIA                 Vincenzo da Filicaia   \n",
       "3   Personali.IV.5.Testo.txt  MIDIA                    Lorenzo Magalotti   \n",
       "4      Poesia.IV.4.Testo.txt  MIDIA               Faustina Maratti Zappi   \n",
       "\n",
       "                                 title    year     period   text type  \\\n",
       "0                   Antichità italiane  1700.0  1700-1750  espositivo   \n",
       "1                               Poesie  1700.0  1700-1750      poesia   \n",
       "2  Lettere inedite a Lorenzo Magalotti  1700.0  1700-1750   personale   \n",
       "3          Lettere odorose (1693-1705)  1700.0  1700-1750   personale   \n",
       "4                               Poesie  1700.0  1700-1750      poesia   \n",
       "\n",
       "                                                text    words  \\\n",
       "0  ﻿IV. 4. Ludovico Antonio Muratori, Antichità i...   8990.0   \n",
       "1  IV. 1. Rime degli Arcadi: Alessi Cillenio (Giu...  10862.0   \n",
       "2  IV. 4. Vincenzo da Filicaia, Lettere inedite a...  10073.0   \n",
       "3  IV. 5. Lorenzo Magalotti, Lettere odorose (169...   8374.0   \n",
       "4  IV. 4. Rime degli Arcadi: Aglauro Cidonia (Fau...   3184.0   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  ﻿iv . 4 . Ludovico Antonio muratori , antichit...   \n",
       "1  iv . 1 . rima del arcadi : alessi cillenio ( G...   \n",
       "2  iv . 4 . Vincenzo da filicaia , lettere inedit...   \n",
       "3  iv . 5 . Lorenzo magalotti , lettere odoroso (...   \n",
       "4  iv . 4 . rima del arcadi : aglauro cidonia ( f...   \n",
       "\n",
       "                              cleaned_tokenized_text  \n",
       "0  [['iv'], [], ['ludovico', 'antonio', 'muratori...  \n",
       "1  [['iv'], [], ['rima', 'arcadi', 'alessi', 'cil...  \n",
       "2  [['iv'], [], ['vincenzo', 'filicaia', 'lettere...  \n",
       "3  [['iv'], [], ['lorenzo', 'magalotti', 'lettere...  \n",
       "4  [['iv'], [], ['rima', 'arcadi', 'aglauro', 'ci...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8d584b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(697296, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f02b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.fillna('')\n",
    "df.lemmatized_text = df.lemmatized_text.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fea6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einzeldataframes für die Zeiträume\n",
    "\n",
    "df_periods = dict(tuple(df.groupby(by='period')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea8a1150",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_periods['1700-1750']\n",
    "df2 = df_periods['1751-1800']\n",
    "df3 = df_periods['1801-1825']\n",
    "df4 = df_periods['1826-1850']\n",
    "df5 = df_periods['1851-1875']\n",
    "df6 = df_periods['1876-1900']\n",
    "df7 = df_periods['1901-1925']\n",
    "df8 = df_periods['1926-1950']\n",
    "df9 = df_periods['1951-1975']\n",
    "df10 = df_periods['1976-2000']\n",
    "df11 = df_periods['2001-2010']\n",
    "df12 = df_periods['2011-2016']\n",
    "df13 = df_periods['2017-2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "524c665b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>text type</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16797</th>\n",
       "      <td>GBspavento.csv</td>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>Vittorio Imbriani</td>\n",
       "      <td>La novellaja fiorentina: Fiabe e novelline ste...</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>1876-1900</td>\n",
       "      <td>prosa letteraria</td>\n",
       "      <td>, E dissegli:–\"Madonna, io son contento \"D''es...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>, e dissegli:–\"madonna , io essere contento \" ...</td>\n",
       "      <td>[['dissegli', 'madonna', 'essere', 'contento',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16798</th>\n",
       "      <td>GBpaura_randomsample.csv</td>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>Vittorio Imbriani</td>\n",
       "      <td>Mastr''Impicca</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>1876-1900</td>\n",
       "      <td>prosa letteraria</td>\n",
       "      <td>... Io.... Lei.... Come qui?\"--rispose la Rosm...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>... io .... lei .... come qui?\"--rispose il ro...</td>\n",
       "      <td>[['qui'], ['rispose', 'rosmunda', 'ancora', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16799</th>\n",
       "      <td>GBpaura_randomsample.csv</td>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>Vittorio Imbriani</td>\n",
       "      <td>Mastr''Impicca</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>1876-1900</td>\n",
       "      <td>prosa letteraria</td>\n",
       "      <td>, perchè alcuni razzi del fuoco d''artificio p...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>, perchè alcun razzo del fuoco d''artificio pr...</td>\n",
       "      <td>[['perchè', 'alcun', 'razzo', 'fuoco', 'd', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16800</th>\n",
       "      <td>GBpaura_randomsample.csv</td>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>Vittorio Imbriani</td>\n",
       "      <td>Mastr''Impicca</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>1876-1900</td>\n",
       "      <td>prosa letteraria</td>\n",
       "      <td>di tutti gli Scaricabarilesi, che non c''è dim...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>di tutto il scaricabarilesi , che non c''è dim...</td>\n",
       "      <td>[['scaricabarilesi', 'dimostrazion', 'd', 'oss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16801</th>\n",
       "      <td>GBpaura_randomsample.csv</td>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>Vittorio Imbriani</td>\n",
       "      <td>Mastr''Impicca</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>1876-1900</td>\n",
       "      <td>prosa letteraria</td>\n",
       "      <td>ne dice la Maestà del despota d''Exibo?\". Don ...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>ne dire il maestà del despota d''exibo ? \" . D...</td>\n",
       "      <td>[['dire', 'maestà', 'despota', 'd', 'exibo'], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            doc     source              author  \\\n",
       "16797            GBspavento.csv  Gutenberg   Vittorio Imbriani   \n",
       "16798  GBpaura_randomsample.csv  Gutenberg   Vittorio Imbriani   \n",
       "16799  GBpaura_randomsample.csv  Gutenberg   Vittorio Imbriani   \n",
       "16800  GBpaura_randomsample.csv  Gutenberg   Vittorio Imbriani   \n",
       "16801  GBpaura_randomsample.csv  Gutenberg   Vittorio Imbriani   \n",
       "\n",
       "                                                   title    year     period  \\\n",
       "16797  La novellaja fiorentina: Fiabe e novelline ste...  1876.0  1876-1900   \n",
       "16798                                     Mastr''Impicca  1876.0  1876-1900   \n",
       "16799                                     Mastr''Impicca  1876.0  1876-1900   \n",
       "16800                                     Mastr''Impicca  1876.0  1876-1900   \n",
       "16801                                     Mastr''Impicca  1876.0  1876-1900   \n",
       "\n",
       "              text type                                               text  \\\n",
       "16797  prosa letteraria  , E dissegli:–\"Madonna, io son contento \"D''es...   \n",
       "16798  prosa letteraria  ... Io.... Lei.... Come qui?\"--rispose la Rosm...   \n",
       "16799  prosa letteraria  , perchè alcuni razzi del fuoco d''artificio p...   \n",
       "16800  prosa letteraria  di tutti gli Scaricabarilesi, che non c''è dim...   \n",
       "16801  prosa letteraria  ne dice la Maestà del despota d''Exibo?\". Don ...   \n",
       "\n",
       "       words                                    lemmatized_text  \\\n",
       "16797   43.0  , e dissegli:–\"madonna , io essere contento \" ...   \n",
       "16798   37.0  ... io .... lei .... come qui?\"--rispose il ro...   \n",
       "16799   40.0  , perchè alcun razzo del fuoco d''artificio pr...   \n",
       "16800   40.0  di tutto il scaricabarilesi , che non c''è dim...   \n",
       "16801   40.0  ne dire il maestà del despota d''exibo ? \" . D...   \n",
       "\n",
       "                                  cleaned_tokenized_text  \n",
       "16797  [['dissegli', 'madonna', 'essere', 'contento',...  \n",
       "16798  [['qui'], ['rispose', 'rosmunda', 'ancora', 'r...  \n",
       "16799  [['perchè', 'alcun', 'razzo', 'fuoco', 'd', 'a...  \n",
       "16800  [['scaricabarilesi', 'dimostrazion', 'd', 'oss...  \n",
       "16801  [['dire', 'maestà', 'despota', 'd', 'exibo'], ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298b52a",
   "metadata": {},
   "source": [
    "## Training von Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b8d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktionen zur Vorbereitung auf das Training\n",
    "# Bereinigung und Tokenisierung\n",
    "\n",
    "def sentence_to_wordlist(raw:str):\n",
    "    \"\"\"\n",
    "    cleans and tokenizes the sentences\n",
    "    \"\"\"\n",
    "    text = re.sub('[^A-Za-z_àÀèÈìÌòÒùÙáÁéÉíÍóÓúÚ]',' ', raw).split()        # Diakritika ans Italienische anpassen                    \n",
    "    filtered_text = [word for word in text if word not in stopwords]        # Stopwörter löschen\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def tokenize_text(raw_text):\n",
    "    \"\"\"\n",
    "    returns a list of lowercase tokenized sentences \n",
    "    \"\"\"\n",
    "    raw_sentences = tokenizer.tokenize(str(raw_text).lower())    \n",
    "    tokenized_sentences = Parallel(n_jobs=-1)(delayed(sentence_to_wordlist)(raw_sentence) for raw_sentence in raw_sentences)\n",
    "    phrases = Phrases(tokenized_sentences)\n",
    "    bigram = Phraser(phrases)\n",
    "    sentences = list(bigram[tokenized_sentences])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167dbe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsparamter setzen\n",
    "\n",
    "vector_size = 300                  # Dimensionality of the word vectors\n",
    "window = 10                        # The maximum distance between the current and predicted word within a sentence\n",
    "min_count = 2                      # (int, optional) – The model ignores all words with total frequency lower than this\n",
    "workers = 4                        # Use these many worker threads to train the model (=faster training with multicore machines)\n",
    "min_alpha = 0.0001                 # Learning rate will linearly drop to min_alpha as training progresses\n",
    "sg = 1                             # Training algorithm: skip-gram if sg=1, otherwise CBOW            \n",
    "seed = 42                          # Reproductivity (42 just because...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab9952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordner anlegen zum Abspeichern von trainierten Modellen\n",
    "\n",
    "if not os.path.exists('../trained_models'):\n",
    "    os.makedirs('../trained_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d041b",
   "metadata": {},
   "source": [
    "### Zeitraum 1: 1700-1750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a28475c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatisierte Texte zu einem String verbinden\n",
    "\n",
    "text1 = ''\n",
    "\n",
    "for i in df1.lemmatized_text:\n",
    "    text1 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd305b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences1 = tokenize_text(text1)         # Bereinigen, Tokenisieren und in Form bringen (Ziel: Liste von tokenisierten Sätzen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d40ec629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['iv'], [], ['ludovico', 'antonio', 'muratori', 'antichità', 'italiano', 'dissertazione', 'gente', 'barbaro', 'assoggettare', 'italia'], ['oggetto', 'ammirazione', 'essere', 'antico', 'tempo', 'roma', 'roma', 'stendere', 'imperio', 'già', 'sopra', 'terra', 'alcun', 'scrittore', 'adulatoriamente', 'scrivere', 'volta', 'sì', 'bene', 'sopra', 'gran_parte', 'tre_parto', 'allora', 'conoscere', 'terra'], ['tanto', 'potenza', 'niuna', 'essere', 'mai', 'giungere', 'precedente', 'monarchia']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0547e952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82676"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f3bbf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Training   \n",
    "\n",
    "w2v1 = Word2Vec(sentences=sentences1,                      \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac18caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spavento', 0.8295801281929016),\n",
       " ('tal_terrore', 0.7880813479423523),\n",
       " ('gran_terrore', 0.7854892611503601),\n",
       " ('barbari', 0.7742155194282532),\n",
       " ('goti', 0.7736718654632568),\n",
       " ('ribellare', 0.7582488656044006),\n",
       " ('de_goti', 0.7563993334770203),\n",
       " ('spaventare', 0.7506831884384155),\n",
       " ('faceano', 0.7486810088157654),\n",
       " ('brescia', 0.745989203453064)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v1.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell speichern\n",
    "\n",
    "w2v1.save(os.path.join('../trained_models', 'w2v1.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3a0bc",
   "metadata": {},
   "source": [
    "### Zeitraum 2: 1751-1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = ''\n",
    "\n",
    "for i in df2.lemmatized_text:\n",
    "    text2 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences2 = tokenize_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee58cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v2 = Word2Vec(sentences=sentences2,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2382df",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v2.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ce604",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v2.save(os.path.join('../trained_models', 'w2v2.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12799bcb",
   "metadata": {},
   "source": [
    "### Zeitraum 3: 1801-1825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31895854",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = ''\n",
    "\n",
    "for i in df3.lemmatized_text:\n",
    "    text3 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences3 = tokenize_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences3[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328353f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v3 = Word2Vec(sentences=sentences3,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc887350",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v3.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v3.save(os.path.join('../trained_models', 'w2v3.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ad1fa",
   "metadata": {},
   "source": [
    "### Zeitraum 4: 1826-1850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = ''\n",
    "\n",
    "for i in df4.lemmatized_text:\n",
    "    text4 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d304d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences4 = tokenize_text(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences4[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305925ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v4 = Word2Vec(sentences=sentences4,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v4.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v4.save(os.path.join('../trained_models', 'w2v4.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adbc52f",
   "metadata": {},
   "source": [
    "### Zeitraum 5: 1851-1875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = ''\n",
    "\n",
    "for i in df5.lemmatized_text:\n",
    "    text5 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences5 = tokenize_text(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences5[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e061e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v5 = Word2Vec(sentences=sentences5,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v5.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b9950",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v5.save(os.path.join('../trained_models', 'w2v5.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8264d8",
   "metadata": {},
   "source": [
    "### Zeitraum 6: 1876-1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text6 = ''\n",
    "\n",
    "for i in df6.lemmatized_text:\n",
    "    text6 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68bdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences6 = tokenize_text(text6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences6[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396efd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v6 = Word2Vec(sentences=sentences6,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v6.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v6.save(os.path.join('../trained_models', 'w2v6.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd33346",
   "metadata": {},
   "source": [
    "### Zeitraum 7: 1901-1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text7 = ''\n",
    "\n",
    "for i in df7.lemmatized_text:\n",
    "    text7 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61dff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences7 = tokenize_text(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences7[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ece89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v7 = Word2Vec(sentences=sentences7,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97859ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v7.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cec0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v7.save(os.path.join('../trained_models', 'w2v7.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54934b98",
   "metadata": {},
   "source": [
    "### Zeitraum 8: 1926-1950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text8 = ''\n",
    "\n",
    "for i in df8.lemmatized_text:\n",
    "    text8 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences8 = tokenize_text(text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7067be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences8[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v8 = Word2Vec(sentences=sentences8,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a676dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v8.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v8.save(os.path.join('../trained_models', 'w2v8.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecf4f1",
   "metadata": {},
   "source": [
    "### Zeitraum 9: 1951-1975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe767d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text9 = ''\n",
    "\n",
    "for i in df9.lemmatized_text:\n",
    "    text9 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences9 = tokenize_text(text9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences9[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v9 = Word2Vec(sentences=sentences9,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v9.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f840943",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v9.save(os.path.join('../trained_models', 'w2v9.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7eb5cd",
   "metadata": {},
   "source": [
    "### Zeitraum 10: 1976-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d34fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text10 = ''\n",
    "\n",
    "for i in df10.lemmatized_text:\n",
    "    text10+= i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences10 = tokenize_text(text10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences10[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe32b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c08bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v10 = Word2Vec(sentences=sentences10,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60227ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v10.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v10.save(os.path.join('../trained_models', 'w2v10.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54297223",
   "metadata": {},
   "source": [
    "### Zeitraum 11: 2001-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text11 = ''\n",
    "\n",
    "for i in df11.lemmatized_text:\n",
    "    text11+= i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences11 = tokenize_text(text11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences11[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b457e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v11 = Word2Vec(sentences=sentences11,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a144e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v11.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v11.save(os.path.join('../trained_models', 'w2v11.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb8c247",
   "metadata": {},
   "source": [
    "### Zeitraum 12: 2011-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text12 = ''\n",
    "\n",
    "for i in df12.lemmatized_text:\n",
    "    text12+= i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences12 = tokenize_text(text12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences12[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e729f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v12 = Word2Vec(sentences=sentences12,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v12.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fc251",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v12.save(os.path.join('../trained_models', 'w2v12.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0d1a1",
   "metadata": {},
   "source": [
    "### Zeitraum 13: 2017-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text13 = ''\n",
    "\n",
    "for i in df13.lemmatized_text:\n",
    "    text13+= i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences13 = tokenize_text(text13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83123aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences13[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7bf223",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb27594",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v13 = Word2Vec(sentences=sentences13,                   \n",
    "                vector_size=vector_size,          \n",
    "                window=window,                \n",
    "                min_count=min_count,              \n",
    "                workers=workers, \n",
    "                min_alpha=min_alpha,         \n",
    "                sg=sg,                     \n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v13.wv.most_similar(positive=['terrore'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80953244",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v13.save(os.path.join('../trained_models', 'w2v13.model'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
